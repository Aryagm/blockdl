# Enhanced BlockDL Layer Configuration
# Version: 2.0.0
# Description: Enhanced modular neural network layers with educational documentation

metadata:
  version: "2.0.0"
  description: "Enhanced modular neural network layers with educational documentation"
  framework: "multi"
  created: "2025-06-03"
  updated: "2025-06-06"
  author: "BlockDL Team"
  license: "MIT"
  settings:
    strict_validation: true
    allow_experimental: false
    default_framework: "keras"
    code_style: "pythonic"

# Category definitions with visual styling
categories:
  input_output:
    name: "Input/Output"
    color: "emerald"
    bg_color: "bg-emerald-50"
    border_color: "border-emerald-200"
    text_color: "text-emerald-700"
    description: "Start and end points of your network"
    icon: "ðŸ”Œ"
    order: 1

  dense:
    name: "Dense Layers"
    color: "blue"
    bg_color: "bg-blue-50"
    border_color: "border-blue-200"
    text_color: "text-blue-700"
    description: "Fully connected layers"
    icon: "ðŸ”—"
    order: 2

  convolutional:
    name: "Convolutional"
    color: "purple"
    bg_color: "bg-purple-50"
    border_color: "border-purple-200"
    text_color: "text-purple-700"
    description: "Conv2D and transpose convolution"
    icon: "ðŸ”²"
    order: 3

  pooling:
    name: "Pooling"
    color: "indigo"
    bg_color: "bg-indigo-50"
    border_color: "border-indigo-200"
    text_color: "text-indigo-700"
    description: "Downsampling and upsampling layers"
    icon: "ðŸŠ"
    order: 4

  transformation:
    name: "Transformation"
    color: "amber"
    bg_color: "bg-amber-50"
    border_color: "border-amber-200"
    text_color: "text-amber-700"
    description: "Shape transformation layers"
    icon: "ðŸ”„"
    order: 5

  activation:
    name: "Activation"
    color: "orange"
    bg_color: "bg-orange-50"
    border_color: "border-orange-200"
    text_color: "text-orange-700"
    description: "Non-linear activation functions"
    icon: "âš¡"
    order: 6

  regularization:
    name: "Regularization"
    color: "rose"
    bg_color: "bg-rose-50"
    border_color: "border-rose-200"
    text_color: "text-rose-700"
    description: "Batch normalization and dropout"
    icon: "ðŸ›¡ï¸"
    order: 7

  sequence:
    name: "Sequence"
    color: "cyan"
    bg_color: "bg-cyan-50"
    border_color: "border-cyan-200"
    text_color: "text-cyan-700"
    description: "RNN and embedding layers"
    icon: "ðŸ“Š"
    order: 8

  merge:
    name: "Merge"
    color: "teal"
    bg_color: "bg-teal-50"
    border_color: "border-teal-200"
    text_color: "text-teal-700"
    description: "Layer combination operations"
    icon: "ðŸ”€"
    order: 9

# Enhanced layer definitions
layers:
  Input:
    metadata:
      category: "input_output"
      icon: "ðŸ“¥"
      description: "Input layer for data"
      tags: ["input", "data"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(1)"
        memory_usage: "Minimal"
        recommended_use: "Start of all neural networks"

    visual:
      handles:
        input: false
        output: true
      default_size: [160, 80]
      resizable: false

    parameters:
      inputType:
        type: "select"
        label: "Input Type"
        description: "Choose the type of input data your model will process"
        default: "image_grayscale"
        options:
          - value: "image_grayscale"
            label: "  Grayscale Image (HÃ—WÃ—1)"
            description: "Single channel images like MNIST"
          - value: "image_color"
            label: "Color Image (HÃ—WÃ—3)"
            description: "RGB color images"
          - value: "image_custom"
            label: "Custom Image (HÃ—WÃ—C)"
            description: "Images with custom number of channels"
          - value: "flat_data"
            label: "Flattened Data (N,)"
            description: "1D vector data"
          - value: "sequence"
            label: "Sequence Data (seq_len, features)"
            description: "Time series or text data"
          - value: "custom"
            label: "Custom Shape"
            description: "Define your own input shape"
        validation:
          required: true
        ui:
          tooltip: "Select the type of data that will enter your network"
          group: "shape"
          order: 1

      height:
        type: "number"
        label: "Height"
        description: "Height dimension of the input image"
        default: 28
        validation:
          min: 1
          max: 10000
        conditional:
          show_when:
            inputType: ["image_grayscale", "image_color", "image_custom"]
        ui:
          group: "shape"
          order: 2

      width:
        type: "number"
        label: "Width"
        description: "Width dimension of the input image"
        default: 28
        validation:
          min: 1
          max: 10000
        conditional:
          show_when:
            inputType: ["image_grayscale", "image_color", "image_custom"]
        ui:
          group: "shape"
          order: 3

      channels:
        type: "number"
        label: "Channels"
        description: "Number of channels (e.g., 1 for grayscale, 3 for RGB)"
        default: 1
        validation:
          min: 1
          max: 512
        conditional:
          show_when:
            inputType: ["image_custom"]
        ui:
          group: "shape"
          order: 4

      flatSize:
        type: "number"
        label: "Size"
        description: "Size of the flattened input vector"
        default: 784
        validation:
          min: 1
          max: 1000000
        conditional:
          show_when:
            inputType: ["flat_data"]
        ui:
          group: "shape"
          order: 5

      seqLength:
        type: "number"
        label: "Sequence Length"
        description: "Length of the input sequence"
        default: 100
        validation:
          min: 1
          max: 10000
        conditional:
          show_when:
            inputType: ["sequence"]
        ui:
          group: "shape"
          order: 6

      features:
        type: "number"
        label: "Features"
        description: "Number of features per sequence step"
        default: 128
        validation:
          min: 1
          max: 10000
        conditional:
          show_when:
            inputType: ["sequence"]
        ui:
          group: "shape"
          order: 7

      customShape:
        type: "text"
        label: "Custom Shape"
        description: "Define a custom input shape (e.g., (784,) or (28, 28, 1))"
        default: "(784,)"
        validation:
          pattern: "^\\([0-9, ]+\\)$"
          error_message: "Shape must be in format (dim1, dim2, ...) like (784,) or (28, 28, 1)"
        conditional:
          show_when:
            inputType: ["custom"]
        ui:
          placeholder: "(784,)"
          group: "shape"
          order: 8

    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false

    frameworks:
      keras:
        import: "Input"
        template: "Input(shape={{computed_shape}})"
        shape_computation: "input_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"

    validation:
      input_shapes: []
      parameter_combinations:
        - inputType: "image_grayscale"
          height: 28
          width: 28
        - inputType: "flat_data"
          flatSize: 784

  Output:
    metadata:
      category: "input_output"
      icon: "ðŸ“¤"
      description: "Output layer for predictions"
      tags: ["output", "classification", "regression"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(n*k) where n=input_size, k=output_size"
        memory_usage: "Low to moderate depending on output size"
        recommended_use: "Final layer for classification or regression tasks"

    visual:
      handles:
        input: true
        output: false
      default_size: [160, 80]
      resizable: false

    parameters:
      outputType:
        type: "select"
        label: "Output Type"
        description: "Choose the type of prediction task"
        default: "multiclass"
        options:
          - value: "multiclass"
            label: "Multi-class Classification (softmax)"
            description: "Choose one class from many (e.g., digit recognition)"
          - value: "binary"
            label: "Binary Classification (sigmoid)"
            description: "Yes/No or True/False predictions"
          - value: "regression"
            label: "Regression (linear)"
            description: "Predict continuous numerical values"
          - value: "multilabel"
            label: "Multi-label Classification (sigmoid)"
            description: "Multiple independent yes/no predictions"
          - value: "custom"
            label: "Custom Configuration"
            description: "Define your own activation and units"
        validation:
          required: true
        ui:
          group: "task"
          order: 1

      numClasses:
        type: "number"
        label: "Number of Classes"
        description: "How many different classes to predict"
        default: 10
        validation:
          min: 2
          max: 10000
        conditional:
          show_when:
            outputType: ["multiclass"]
        ui:
          group: "task"
          order: 2

      units:
        type: "number"
        label: "Output Units"
        description: "Number of output neurons"
        default: 1
        validation:
          min: 1
          max: 10000
        conditional:
          show_when:
            outputType: ["custom", "multilabel", "regression"]
        ui:
          group: "task"
          order: 3

      activation:
        type: "select"
        label: "Activation"
        description: "Activation function for the output layer"
        default: "softmax"
        options:
          - value: "softmax"
            label: "Softmax"
            description: "For multi-class classification"
          - value: "sigmoid"
            label: "Sigmoid"
            description: "For binary or multi-label classification"
          - value: "linear"
            label: "Linear"
            description: "For regression tasks"
          - value: "tanh"
            label: "Tanh"
            description: "Outputs between -1 and 1"
          - value: "relu"
            label: "ReLU"
            description: "Non-negative outputs"
        conditional:
          show_when:
            outputType: ["custom"]
        ui:
          group: "activation"
          order: 4

      threshold:
        type: "number"
        label: "Decision Threshold"
        description: "Threshold for binary classification decision"
        default: 0.5
        validation:
          min: 0
          max: 1
          step: 0.1
        conditional:
          show_when:
            outputType: ["binary"]
        ui:
          group: "threshold"
          order: 5

    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: true

    frameworks:
      keras:
        import: "Dense"
        template: "Dense({{computed_units}}, activation='{{computed_activation}}')"
        shape_computation: "dense_layer"
        dependencies: ["tensorflow"]

  Dense:
    metadata:
      category: "dense"
      icon: "ðŸ”—"
      description: "Fully connected layer"
      tags: ["dense", "fully_connected"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(n*m) where n=input_size, m=units"
        memory_usage: "High for large layers"
        recommended_use: "Feature learning and final classification layers"

    visual:
      handles:
        input: true
        output: true
      default_size: [140, 80]
      resizable: true

    parameters:
      units:
        type: "number"
        label: "Units"
        description: "Number of output neurons"
        default: 128
        validation:
          min: 1
          max: 10000
          required: true
        ui:
          tooltip: "Number of neurons in this dense layer"
          group: "architecture"
          order: 1

      activation:
        type: "select"
        label: "Activation (optional)"
        description: "Activation function to apply"
        default: "none"
        options:
          - value: "none"
            label: "None"
            description: "Linear activation (no transformation)"
          - value: "relu"
            label: "ReLU"
            description: "Most common activation for hidden layers"
          - value: "sigmoid"
            label: "Sigmoid"
            description: "Outputs between 0 and 1"
          - value: "tanh"
            label: "Tanh"
            description: "Outputs between -1 and 1"
          - value: "softmax"
            label: "Softmax"
            description: "For probability distributions"
          - value: "linear"
            label: "Linear"
            description: "Same as none, explicit linear"
        ui:
          group: "activation"
          order: 2

      multiplier:
        type: "number"
        label: "Repeat (x times)"
        description: "Repeat this layer multiple times"
        default: 1
        validation:
          min: 1
          max: 20
        ui:
          tooltip: "Stack multiple identical dense layers"
          group: "architecture"
          order: 3

    features:
      supports_multiplier: true
      supports_batch_processing: true
      supports_gradient_checkpointing: true
      trainable: true

    frameworks:
      keras:
        import: "Dense"
        template: "Dense({{units}}{{activation_suffix}})"
        shape_computation: "dense_layer"
        dependencies: ["tensorflow"]

  Conv2D:
    metadata:
      category: "convolutional"
      icon: "ðŸ”²"
      description: "2D convolution layer"
      tags: ["convolution", "cnn", "2d"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(H*W*C*F*KÂ²) where H,W=output size, C=input channels, F=filters, K=kernel size"
        memory_usage: "Moderate, depends on filter count"
        recommended_use: "Feature extraction from images and 2D data"

    visual:
      handles:
        input: true
        output: true
      default_size: [160, 100]
      resizable: true

    parameters:
      filters:
        type: "number"
        label: "Filters"
        description: "Number of output filters/feature maps"
        default: 32
        validation:
          min: 1
          max: 1024
          required: true
        ui:
          tooltip: "More filters = more features detected"
          group: "architecture"
          order: 1

      kernel_size:
        type: "text"
        label: "Kernel Size"
        description: "Size of convolution kernel"
        default: "(3,3)"
        validation:
          pattern: "^\\([0-9]+,[0-9]+\\)$"
          error_message: "Kernel size must be in format (height,width) like (3,3)"
        ui:
          placeholder: "(3,3)"
          tooltip: "Common sizes: (1,1), (3,3), (5,5), (7,7)"
          group: "convolution"
          order: 2

      strides:
        type: "text"
        label: "Strides"
        description: "Stride of convolution"
        default: "(1,1)"
        validation:
          pattern: "^\\([0-9]+,[0-9]+\\)$"
          error_message: "Strides must be in format (height,width) like (1,1)"
        ui:
          placeholder: "(1,1)"
          tooltip: "(1,1) = no downsampling, (2,2) = 2x downsampling"
          group: "convolution"
          order: 3

      padding:
        type: "select"
        label: "Padding"
        description: "Padding strategy"
        default: "same"
        options:
          - value: "valid"
            label: "Valid"
            description: "No padding, output size reduced"
          - value: "same"
            label: "Same"
            description: "Padding to keep same output size"
        ui:
          group: "convolution"
          order: 4

      multiplier:
        type: "number"
        label: "Repeat (x times)"
        description: "Stack multiple Conv2D layers"
        default: 1
        validation:
          min: 1
          max: 10
        ui:
          group: "architecture"
          order: 5

    features:
      supports_multiplier: true
      supports_batch_processing: true
      supports_gradient_checkpointing: true
      trainable: true

    frameworks:
      keras:
        import: "Conv2D"
        template: "Conv2D({{filters}}, kernel_size={{kernel_size}}, strides={{strides}}, padding='{{padding}}')"
        shape_computation: "conv2d_layer"
        dependencies: ["tensorflow"]

  Conv2DTranspose:
    metadata:
      category: "convolutional"
      icon: "ðŸ”³"
      description: "2D transpose convolution layer (deconvolution), often used for upsampling feature maps."
      tags: ["transpose", "deconvolution", "upsampling", "cnn"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "Similar to Conv2D, depends on output size"
        memory_usage: "Moderate"
        recommended_use: "Upsampling in autoencoders, GANs, or segmentation models."
    visual:
      handles:
        input: true
        output: true
      default_size: [160, 100]
      resizable: true
    parameters:
      filters:
        type: "number"
        label: "Filters"
        description: "Number of output filters/feature maps."
        default: 32
        validation:
          min: 1
          max: 1024
          required: true
        ui:
          tooltip: "Number of channels in the output."
          group: "architecture"
          order: 1
      kernel_size:
        type: "text"
        label: "Kernel Size"
        description: "Size of the convolution kernel (height, width)."
        default: "(3,3)"
        validation:
          pattern: "^\\\\([0-9]+,[0-9]+\\\\)$"
          error_message: "Kernel size must be in format (height,width) like (3,3)"
          required: true
        ui:
          placeholder: "(3,3)"
          tooltip: "Size of the kernel to use for deconvolution."
          group: "convolution"
          order: 2
      strides:
        type: "text"
        label: "Strides"
        description: "Strides of the convolution (height, width)."
        default: "(2,2)"
        validation:
          pattern: "^\\\\([0-9]+,[0-9]+\\\\)$"
          error_message: "Strides must be in format (height,width) like (2,2)"
          required: true
        ui:
          placeholder: "(2,2)"
          tooltip: "Typically (2,2) for 2x upsampling."
          group: "convolution"
          order: 3
      padding:
        type: "select"
        label: "Padding"
        description: "Padding strategy ('valid' or 'same')."
        default: "same"
        options:
          - value: "valid"
            label: "Valid"
            description: "No padding."
          - value: "same"
            label: "Same"
            description: "Padding such that output has the desired dimensions, often related to stride."
        validation:
          required: true
        ui:
          group: "convolution"
          order: 4
    features:
      supports_multiplier: false
      supports_batch_processing: true
      supports_gradient_checkpointing: false
      trainable: true
    frameworks:
      keras:
        import: "Conv2DTranspose"
        template: |
          Conv2DTranspose(filters={{filters}}, kernel_size={{kernel_size}}, strides={{strides}}, padding='{{padding}}')
        shape_computation: "conv2d_transpose_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[224, 224, 3]]

  MaxPool2D:
    metadata:
      category: "pooling"
      icon: "â¬‡ï¸"
      description: "Max pooling operation for 2D spatial data. Reduces dimensionality."
      tags: ["pooling", "downsampling", "cnn"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(H*W*C) where H,W,C are input dimensions"
        memory_usage: "Low"
        recommended_use: "Downsampling feature maps, reducing computational load, providing translation invariance."
    visual:
      handles:
        input: true
        output: true
      default_size: [140, 80]
      resizable: true
    parameters:
      pool_size:
        type: "text"
        label: "Pool Size"
        description: "Size of the pooling window (height, width)."
        default: "(2,2)"
        validation:
          pattern: "^\\\\([0-9]+,[0-9]+\\\\)$"
          error_message: "Pool size must be in format (height,width) like (2,2)"
          required: true
        ui:
          placeholder: "(2,2)"
          tooltip: "Typically (2,2) to halve dimensions."
          group: "pooling"
          order: 1
      strides:
        type: "text"
        label: "Strides (optional)"
        description: "Strides of the pooling operation. Defaults to pool_size if not specified."
        default: ""
        validation:
          pattern: "^\\\\([0-9]+,[0-9]+\\\\)$"
          required: false
        ui:
          placeholder: "(2,2)"
          tooltip: "Defaults to pool_size if left empty."
          group: "pooling"
          order: 2
      padding:
        type: "select"
        label: "Padding"
        description: "Padding strategy ('valid' or 'same')."
        default: "valid"
        options:
          - value: "valid"
            label: "Valid"
            description: "No padding."
          - value: "same"
            label: "Same"
            description: "Padding to maintain dimensions (less common for max pooling unless strides=1)."
        validation:
          required: true
        ui:
          group: "pooling"
          order: 3
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false
    frameworks:
      keras:
        import: "MaxPool2D"
        template: |
          MaxPool2D(pool_size={{pool_size}}{% if strides %}, strides={{strides}}{% endif %}, padding='{{padding}}')
        shape_computation: "maxpool2d_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[224, 224, 3]]

  GlobalAveragePooling2D:
    metadata:
      category: "pooling"
      icon: "ðŸŒ"
      description: "Global average pooling operation for 2D spatial data. Reduces each feature map to a single number."
      tags: ["pooling", "global", "cnn", "dimensionality_reduction"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(H*W*C)"
        memory_usage: "Low"
        recommended_use: "Reducing feature maps before a dense layer, helps prevent overfitting."
    visual:
      handles:
        input: true
        output: true
      default_size: [180, 80]
      resizable: false
    parameters:
      keepdims:
        type: "boolean"
        label: "Keep Dimensions"
        description: "Whether to keep the spatial dimensions (as 1x1) or remove them."
        default: false
        ui:
          tooltip: "If true, output shape is (batch, 1, 1, channels)."
          group: "pooling"
          order: 1
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false
    frameworks:
      keras:
        import: "GlobalAveragePooling2D"
        template: |
          GlobalAveragePooling2D({% if keepdims %}keepdims={{keepdims}}{% endif %})
        shape_computation: "global_avg_pool_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[224, 224, 3]]

  UpSampling2D:
    metadata:
      category: "pooling"
      icon: "â¬†ï¸"
      description: "Upsampling layer for 2D inputs. Repeats rows and columns of the data."
      tags: ["upsampling", "cnn", "autoencoder", "gan"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "Low, involves data repetition"
        memory_usage: "Increases with upsampling factor"
        recommended_use: "Increasing spatial resolution in generative models or autoencoders."
    visual:
      handles:
        input: true
        output: true
      default_size: [160, 80]
      resizable: true
    parameters:
      size:
        type: "text"
        label: "Upsampling Factor"
        description: "Factor by which to upscale dimensions (height_factor, width_factor) or single int for both."
        default: "(2,2)"
        validation:
          pattern: "^(?:\\\\([0-9]+,[0-9]+\\\\)|[0-9]+)$"
          error_message: "Size must be an integer or a tuple of 2 integers e.g., 2 or (2,2)"
          required: true
        ui:
          placeholder: "(2,2) or 2"
          tooltip: "e.g., (2,2) or 2 doubles height and width."
          group: "upsampling"
          order: 1
      interpolation:
        type: "select"
        label: "Interpolation Method"
        description: "The interpolation method."
        default: "nearest"
        options:
          - value: "nearest"
            label: "Nearest Neighbor"
          - value: "bilinear"
            label: "Bilinear"
        validation:
          required: true
        ui:
          group: "upsampling"
          order: 2
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false
    frameworks:
      keras:
        import: "UpSampling2D"
        template: |
          UpSampling2D(size={{size}}, interpolation='{{interpolation}}')
        shape_computation: "upsampling2d_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[224, 224, 3]]

  Flatten:
    metadata:
      category: "transformation"
      icon: "ðŸ“"
      description: "Flattens the input. Does not affect the batch size."
      tags: ["flatten", "reshape", "cnn", "dense_transition"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(1) or O(N) for data movement, effectively very low"
        memory_usage: "None, reshapes in place or creates a view"
        recommended_use: "Transitioning from convolutional/pooling layers to dense layers."
    visual:
      handles:
        input: true
        output: true
      default_size: [120, 60]
      resizable: false
    parameters: {}
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false
    frameworks:
      keras:
        import: "Flatten"
        template: |
          Flatten()
        shape_computation: "flatten_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[28, 28, 512]]

  Activation:
    metadata:
      category: "activation"
      icon: "âš¡"
      description: "Applies an activation function to an output. Can be a standalone layer."
      tags: ["activation", "nonlinearity"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(N) where N is number of elements"
        memory_usage: "Low, often in-place"
        recommended_use: "Applying non-linearities, often included in other layers but can be explicit."
    visual:
      handles:
        input: true
        output: true
      default_size: [120, 70]
      resizable: false
    parameters:
      activation_function:
        type: "select"
        label: "Activation Type"
        description: "The type of activation function to apply."
        default: "relu"
        options:
          - value: "relu"
            label: "ReLU"
            description: "Rectified Linear Unit (max(0, x))"
          - value: "sigmoid"
            label: "Sigmoid"
            description: "Sigmoid function (1 / (1 + exp(-x)))"
          - value: "tanh"
            label: "Tanh"
            description: "Hyperbolic Tangent (tanh(x))"
          - value: "softmax"
            label: "Softmax"
            description: "Softmax function (for probability distributions)"
          - value: "linear"
            label: "Linear"
            description: "Linear activation (f(x) = x)"
          - value: "elu"
            label: "ELU"
            description: "Exponential Linear Unit"
          - value: "selu"
            label: "SELU"
            description: "Scaled Exponential Linear Unit"
        validation:
          required: true
        ui:
          group: "function"
          order: 1
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false
    frameworks:
      keras:
        import: "Activation"
        template: |
          Activation('{{activation_function}}')
        shape_computation: "preserve_shape"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[100]]

  BatchNormalization:
    metadata:
      category: "regularization"
      icon: "ðŸ“Š"
      description: "Batch Normalization layer. Normalizes the activations of the previous layer at each batch."
      tags: ["normalization", "regularization", "cnn", "training_stability"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(N) for forward/backward pass, plus tracking means/variances"
        memory_usage: "Stores moving mean/variance, scale/offset parameters"
        recommended_use: "Improve training speed and stability, reduce internal covariate shift."
    visual:
      handles:
        input: true
        output: true
      default_size: [180, 80]
      resizable: true
    parameters:
      axis:
        type: "number"
        label: "Axis"
        description: "The axis that should be normalized (typically the features axis, -1)."
        default: -1
        validation:
          required: false
        ui:
          tooltip: "Integer, the axis that should be normalized. Typically -1 for the features axis."
          group: "normalization"
          order: 1
      momentum:
        type: "number"
        label: "Momentum"
        description: "Momentum for the moving average used for mean and variance."
        default: 0.99
        validation:
          min: 0
          max: 1
          step: 0.001
          required: false
        ui:
          group: "normalization"
          order: 2
      epsilon:
        type: "number"
        label: "Epsilon"
        description: "Small float added to variance to avoid dividing by zero."
        default: 0.001
        validation:
          min: 0
          required: false
        ui:
          group: "normalization"
          order: 3
      center:
        type: "boolean"
        label: "Center (use beta)"
        description: "If True, add offset of beta to normalized tensor."
        default: true
        ui:
          group: "normalization"
          order: 4
      scale:
        type: "boolean"
        label: "Scale (use gamma)"
        description: "If True, multiply by gamma."
        default: true
        ui:
          group: "normalization"
          order: 5
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: true
    frameworks:
      keras:
        import: "BatchNormalization"
        template: |
          BatchNormalization(axis={{axis}}, momentum={{momentum}}, epsilon={{epsilon}}, center={{center}}, scale={{scale}})
        shape_computation: "preserve_shape"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[100, 128]]

  Dropout:
    metadata:
      category: "regularization"
      icon: "ðŸŽ²"
      description: "Applies Dropout to the input. Helps prevent overfitting by randomly setting input units to 0."
      tags: ["dropout", "regularization", "overfitting_prevention"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(N) during training, O(1) or O(N) with scaling during inference (often no-op)"
        memory_usage: "Low, may need a mask during training"
        recommended_use: "Regularizing layers, especially dense layers or before final layers."
    visual:
      handles:
        input: true
        output: true
      default_size: [120, 70]
      resizable: false
    parameters:
      rate:
        type: "number"
        label: "Dropout Rate"
        description: "Fraction of the input units to drop."
        default: 0.2
        validation:
          min: 0
          max: 1
          step: 0.01
          required: true
        ui:
          tooltip: "e.g., 0.2 means 20% of units will be dropped. Must be > 0 and < 1."
          group: "dropout"
          order: 1
      noise_shape:
        type: "text"
        label: "Noise Shape (optional)"
        description: "1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input."
        default: ""
        validation:
          pattern: "^\\\\([0-9, ]*\\\\)$"
          required: false
        ui:
          placeholder: "(batch_size, 1, features)"
          tooltip: "Shape for dropout mask, e.g., to drop entire feature maps."
          group: "dropout"
          order: 2
      seed:
        type: "number"
        label: "Seed (optional)"
        description: "A Python integer to use as random seed."
        validation:
          required: false
        ui:
          tooltip: "Random seed for reproducibility."
          group: "dropout"
          order: 3
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false
    frameworks:
      keras:
        import: "Dropout"
        template: |
          Dropout({{rate}}{% if noise_shape %}, noise_shape={{noise_shape}}{% endif %}{% if seed is not none %}, seed={{seed}}{% endif %})
        shape_computation: "preserve_shape"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[100]]

  Embedding:
    metadata:
      category: "sequence"
      icon: "ðŸ“š"
      description: "Turns positive integers (indexes) into dense vectors of fixed size. Used for NLP, categorical features."
      tags: ["embedding", "text", "nlp", "categorical_data", "sequence"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "O(input_length) for lookup during forward pass"
        memory_usage: "O(input_dim * output_dim) for embedding matrix"
        recommended_use: "Representing words or categorical variables as dense vectors."
    visual:
      handles:
        input: true
        output: true
      default_size: [180, 90]
      resizable: true
    parameters:
      input_dim:
        type: "number"
        label: "Vocabulary Size (Input Dim)"
        description: "Size of the vocabulary, i.e. maximum integer index + 1."
        default: 10000
        validation:
          min: 1
          required: true
        ui:
          tooltip: "Total number of unique words/tokens in your vocabulary."
          group: "embedding_params"
          order: 1
      output_dim:
        type: "number"
        label: "Embedding Dimension (Output Dim)"
        description: "Dimension of the dense embedding."
        default: 128
        validation:
          min: 1
          required: true
        ui:
          tooltip: "Size of the vector that will represent each word/token."
          group: "embedding_params"
          order: 2
      input_length:
        type: "number"
        label: "Input Sequence Length (optional)"
        description: "Length of input sequences, if all are same length. Required if first layer."
        validation:
          min: 1
          required: false
        ui:
          tooltip: "Maximum length of the input sequences. Can be inferred if not first layer."
          group: "embedding_params"
          order: 3
      mask_zero:
        type: "boolean"
        label: "Mask Zero"
        description: "Whether or not the input value 0 is a special padding value that should be masked out."
        default: false
        ui:
          tooltip: "Set to True if 0 is used for padding in sequences."
          group: "embedding_params"
          order: 4
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: true
    frameworks:
      keras:
        import: "Embedding"
        template: |
          Embedding(input_dim={{input_dim}}, output_dim={{output_dim}}{% if input_length is not none %}, input_length={{input_length}}{% endif %}, mask_zero={{mask_zero}})
        shape_computation: "embedding_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[20, 50]]

  LSTM:
    metadata:
      category: "sequence"
      icon: "ðŸ”„"
      description: "Long Short-Term Memory layer, a type of Recurrent Neural Network (RNN)."
      tags: ["lstm", "rnn", "sequence", "time_series", "nlp"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "High, O(units^2 * sequence_length)"
        memory_usage: "High, stores weights and activations"
        recommended_use: "Processing sequential data like text, time series, speech."
    visual:
      handles:
        input: true
        output: true
      default_size: [160, 90]
      resizable: true
    parameters:
      units:
        type: "number"
        label: "Units"
        description: "Dimensionality of the output space (number of LSTM units)."
        default: 128
        validation:
          min: 1
          required: true
        ui:
          tooltip: "Number of hidden units in the LSTM cell."
          group: "architecture"
          order: 1
      activation:
        type: "select"
        label: "Activation"
        description: "Activation function to use for the cell state."
        default: "tanh"
        options:
          - value: "tanh"
            label: "Tanh"
          - value: "sigmoid"
            label: "Sigmoid"
          - value: "relu"
            label: "ReLU"
          - value: "linear"
            label: "Linear"
        ui:
          group: "architecture"
          order: 2
      recurrent_activation:
        type: "select"
        label: "Recurrent Activation"
        description: "Activation function to use for the recurrent step."
        default: "sigmoid"
        options:
          - value: "sigmoid"
            label: "Sigmoid"
          - value: "tanh"
            label: "Tanh"
          - value: "relu"
            label: "ReLU"
          - value: "linear"
            label: "Linear"
        ui:
          group: "architecture"
          order: 3
      return_sequences:
        type: "boolean"
        label: "Return Sequences"
        description: "Whether to return the last output in the output sequence, or the full sequence."
        default: false
        ui:
          tooltip: "True to return full sequence for stacking LSTMs, False for last output."
          group: "output_shape"
          order: 4
      return_state:
        type: "boolean"
        label: "Return State"
        description: "Whether to return the last state in addition to the output."
        default: false
        ui:
          group: "output_shape"
          order: 5
      multiplier:
        type: "number"
        label: "Repeat (x times)"
        description: "Stack multiple identical LSTM layers. Note: if repeating, ensure return_sequences=true for intermediate layers."
        default: 1
        validation:
          min: 1
          max: 10
        ui:
          tooltip: "Number of times to repeat this LSTM layer configuration."
          group: "architecture"
          order: 6
    features:
      supports_multiplier: true
      supports_batch_processing: true
      supports_gradient_checkpointing: true
      trainable: true
    frameworks:
      keras:
        import: "LSTM"
        template: |
          LSTM(units={{units}}, activation='{{activation}}', recurrent_activation='{{recurrent_activation}}', return_sequences={{return_sequences}}, return_state={{return_state}})
        shape_computation: "lstm_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[20, 50, 128]]

  GRU:
    metadata:
      category: "sequence"
      icon: "ðŸ”"
      description: "Gated Recurrent Unit layer, another type of Recurrent Neural Network (RNN)."
      tags: ["gru", "rnn", "sequence", "time_series", "nlp"]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "Slightly less than LSTM, O(units^2 * sequence_length)"
        memory_usage: "Slightly less than LSTM"
        recommended_use: "Similar to LSTM, often trains faster with comparable performance on some tasks."
    visual:
      handles:
        input: true
        output: true
      default_size: [160, 90]
      resizable: true
    parameters:
      units:
        type: "number"
        label: "Units"
        description: "Dimensionality of the output space (number of GRU units)."
        default: 128
        validation:
          min: 1
          required: true
        ui:
          tooltip: "Number of hidden units in the GRU cell."
          group: "architecture"
          order: 1
      activation:
        type: "select"
        label: "Activation"
        description: "Activation function to use for the cell state."
        default: "tanh"
        options:
          - value: "tanh"
            label: "Tanh"
          - value: "sigmoid"
            label: "Sigmoid"
          - value: "relu"
            label: "ReLU"
          - value: "linear"
            label: "Linear"
        ui:
          group: "architecture"
          order: 2
      recurrent_activation:
        type: "select"
        label: "Recurrent Activation"
        description: "Activation function to use for the recurrent step."
        default: "sigmoid"
        options:
          - value: "sigmoid"
            label: "Sigmoid"
          - value: "tanh"
            label: "Tanh"
          - value: "relu"
            label: "ReLU"
          - value: "linear"
            label: "Linear"
        ui:
          group: "architecture"
          order: 3
      return_sequences:
        type: "boolean"
        label: "Return Sequences"
        description: "Whether to return the last output in the output sequence, or the full sequence."
        default: false
        ui:
          tooltip: "True to return full sequence for stacking GRUs, False for last output."
          group: "output_shape"
          order: 4
      return_state:
        type: "boolean"
        label: "Return State"
        description: "Whether to return the last state in addition to the output."
        default: false
        ui:
          group: "output_shape"
          order: 5
      reset_after:
        type: "boolean"
        label: "Reset After"
        description: "GRU convention (reset gate after matrix multiplication or before)."
        default: true
        ui:
          group: "architecture"
          order: 6
    features:
      supports_multiplier: false
      supports_batch_processing: true
      supports_gradient_checkpointing: true
      trainable: true
    frameworks:
      keras:
        import: "GRU"
        template: |
          GRU(units={{units}}, activation='{{activation}}', recurrent_activation='{{recurrent_activation}}', return_sequences={{return_sequences}}, return_state={{return_state}}, reset_after={{reset_after}})
        shape_computation: "gru_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[20, 50, 128]]

  Merge:
    metadata:
      category: "merge"
      icon: "ðŸ”€"
      description: "Merges a list of inputs using a specified mode (e.g., concatenate, add)."
      tags:
        [
          "merge",
          "concatenate",
          "add",
          "multiply",
          "average",
          "maximum",
          "functional_api",
        ]
      info: "/docs/general-info.md"
      version: "2.0.0"
      performance:
        computational_complexity: "Varies by mode (e.g., Concat is low, Add/Multiply are O(N))"
        memory_usage: "Varies by mode and output size"
        recommended_use: "Combining multiple branches in a model, feature fusion."
    visual:
      handles:
        input: true
        output: true
        multiple_inputs: true
      default_size: [140, 80]
      resizable: false
    parameters:
      mode:
        type: "select"
        label: "Merge Mode"
        description: "The way inputs should be merged."
        default: "concatenate"
        options:
          - value: "concatenate"
            label: "Concatenate"
            description: "Join tensors along an axis."
          - value: "add"
            label: "Add"
            description: "Element-wise addition."
          - value: "multiply"
            label: "Multiply"
            description: "Element-wise multiplication."
          - value: "average"
            label: "Average"
            description: "Element-wise average."
          - value: "maximum"
            label: "Maximum"
            description: "Element-wise maximum."
        validation:
          required: true
        ui:
          group: "merge_options"
          order: 1
      axis:
        type: "number"
        label: "Axis (for Concatenate)"
        description: "Axis along which to concatenate. Default is -1 (last axis)."
        default: -1
        validation:
          required: false
        conditional:
          show_when:
            mode: ["concatenate"]
        ui:
          tooltip: "The axis along which the tensors will be concatenated."
          group: "merge_options"
          order: 2
    features:
      supports_multiplier: false
      supports_batch_processing: true
      trainable: false
    frameworks:
      keras:
        import: "Concatenate, Add, Multiply, Average, Maximum"
        template: "{% if mode == 'concatenate' %}Concatenate(){% endif %}{% if mode == 'add' %}Add(){% endif %}{% if mode == 'multiply' %}Multiply(){% endif %}{% if mode == 'average' %}Average(){% endif %}{% if mode == 'maximum' %}Maximum(){% endif %}"
        shape_computation: "merge_layer"
        dependencies: ["tensorflow"]
        version_constraints:
          min: "2.0.0"
    validation:
      input_shapes: [[100, 64], [100, 64]]
  
  

# Global template definitions for reuse
global_templates:
  basic_activation_template:
    base: "{{layer_name}}({{params}}{{activation_suffix}})"
    variables:
      activation_suffix:
        source: "activation"
        type: "string"
        transform: "activation_transform"
        default: ""
    description: "Standard template for layers with optional activation"

# Global formula definitions for shape computation
global_formulas:
  conv_output_size:
    type: "formula"
    expression: "floor((input_size + 2*padding - kernel_size) / stride) + 1"
    variables:
      input_size: "Height or width of input"
      padding: "Padding applied"
      kernel_size: "Size of convolution kernel"
      stride: "Stride of convolution"
    description: "Calculate output size for convolution operations"

# Global validation patterns
global_validators:
  tuple_pattern: "^\\([0-9, ]+\\)$"
  positive_integer: "^[1-9][0-9]*$"
  probability: "^(0(\\.[0-9]+)?|1(\\.0+)?)$"

# Enhanced Shape Computation System
# This section defines enhanced validation and error messaging for shape computation
enhanced_shape_computation:
  dense_layer:
    shape_computation: "dense_layer"
    shape_validation:
      type: "enhanced"
      rules:
        - condition: "input_dims > 2"
          warning: "Dense layer receiving {input_dims}D input ({input_shape}) will be automatically flattened. Consider adding explicit Flatten layer before Dense for clarity."
        - condition: "input_dims === 1 && input_shape[0] > 10000"
          warning: "Large input size ({input_shape[0]}) may cause memory issues. Consider using dimensionality reduction."
        - condition: "units > 4096"
          warning: "Very large Dense layer ({units} units) may be slow to train and prone to overfitting."
      error_messages:
        invalid_input: "Dense layer requires at least 1D input, got invalid shape: {input_shape}"
        invalid_units: "Dense layer units must be a positive integer, got: {units}"

  conv2d_layer:
    shape_computation: "conv2d_layer"
    shape_validation:
      type: "enhanced"
      rules:
        - condition: "input_dims !== 3"
          error: "Conv2D requires 3D input (height, width, channels), got {input_dims}D input: {input_shape}. Add reshape or use Dense layer for 1D/2D inputs."
        - condition: "input_shape[0] < kernel_size[0] || input_shape[1] < kernel_size[1]"
          error: "Input size ({input_shape[0]}x{input_shape[1]}) is smaller than kernel size ({kernel_size[0]}x{kernel_size[1]}). Reduce kernel size or use padding."
        - condition: "filters < 1"
          error: "Conv2D filters must be positive, got: {filters}"
      error_messages:
        invalid_kernel: "Invalid kernel size format: {kernel_size}. Use format like '(3,3)'"
        invalid_strides: "Invalid strides format: {strides}. Use format like '(1,1)'"

  maxpool2d_layer:
    shape_computation: "maxpool2d_layer"
    shape_validation:
      type: "enhanced"
      rules:
        - condition: "input_dims !== 3"
          error: "MaxPool2D requires 3D input (height, width, channels), got {input_dims}D input: {input_shape}"
        - condition: "input_shape[0] < pool_size[0] || input_shape[1] < pool_size[1]"
          error: "Input size ({input_shape[0]}x{input_shape[1]}) is smaller than pool size ({pool_size[0]}x{pool_size[1]})"
      error_messages:
        invalid_pool_size: "Invalid pool size format: {pool_size}. Use format like '(2,2)'"

  flatten_layer:
    shape_computation: "flatten_layer"
    shape_validation:
      type: "enhanced"
      rules:
        - condition: "input_dims < 2"
          warning: "Flattening 1D input has no effect"
        - condition: "total_elements > 100000"
          warning: "Flattening large tensor ({total_elements} elements) may cause memory issues"

  lstm_layer:
    shape_computation: "lstm_layer"
    shape_validation:
      type: "enhanced"
      rules:
        - condition: "input_dims !== 3"
          error: "LSTM requires 3D input (batch, timesteps, features), got {input_dims}D input: {input_shape}"
        - condition: "units < 1"
          error: "LSTM units must be positive, got: {units}"

  embedding_layer:
    shape_computation: "embedding_layer"
    shape_validation:
      type: "enhanced"
      rules:
        - condition: "input_dims > 2"
          error: "Embedding layer accepts 1D or 2D input, got {input_dims}D input: {input_shape}"
        - condition: "output_dim < 1"
          error: "Embedding output_dim must be positive, got: {output_dim}"

# Enhanced error message templates
enhanced_error_templates:
  shape_mismatch: "Shape mismatch in {layer_type}: expected {expected_shape}, got {actual_shape}"
  dimension_error: "Dimension error in {layer_type}: expected {expected_dims}D input, got {actual_dims}D"
  parameter_error: "Parameter error in {layer_type}: {parameter} = {value} is invalid. {suggestion}"
  memory_warning: "Memory warning in {layer_type}: {reason}. Consider {suggestion}"
  compatibility_error: "Compatibility error: {layer_type} cannot connect to {previous_layer_type} with shape {previous_shape}"

# Enhanced validation settings
enhanced_validation_settings:
  strict_mode: true
  show_warnings: true
  show_suggestions: true
  max_error_details: 3
  warning_thresholds:
    large_tensor_elements: 100000
    large_dense_units: 4096
    deep_network_layers: 50
